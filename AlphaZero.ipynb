{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study notes\n",
    "\n",
    "Alphazero on a game \"5 in a Row\"\n",
    "\n",
    "```\n",
    " initialize neural network\n",
    " for each iteration i do:\n",
    "    initialize s0\n",
    "    for each step t until termination step T do:\n",
    "        while computational resource remains do:\n",
    "            get_move_probs() [check notes in MCTSplayer]\n",
    "        end\n",
    "        play: get_action()\n",
    "    end\n",
    "    \n",
    "    score the game to give a final reward\n",
    "    \n",
    "    for each step t in last game do:\n",
    "        z <-- +,- rT\n",
    "           store data as (st,pit.zt)\n",
    "    end\n",
    "    \n",
    "    sample data (s,pi,z) uniformly among all time-steps of the last\n",
    "    iterations of self-play\n",
    "    \n",
    "    adjust the neural network (p,v) = f(s)\n",
    "    minimize: v and z\n",
    "    maximize: p and pi\n",
    "end\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game: 5 in a row 五子棋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Board(object):\n",
    "    \"\"\"\n",
    "    board for the game\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.width = int(kwargs.get('width', 8))\n",
    "        self.height = int(kwargs.get('height', 8))\n",
    "        self.states = {} # board states, key:move as location on the board, value:player as pieces type\n",
    "        self.n_in_row = int(kwargs.get('n_in_row', 5)) # need how many pieces in a row to win\n",
    "        self.players = [1, 2] # player1 and player2\n",
    "        \n",
    "    def init_board(self, start_player=0):\n",
    "        if self.width < self.n_in_row or self.height < self.n_in_row:\n",
    "            raise Exception('board width and height can not less than %d' % self.n_in_row)\n",
    "        self.current_player = self.players[start_player]  # start player        \n",
    "        self.availables = list(range(self.width * self.height)) # available moves \n",
    "        self.states = {} # board states, key:move as location on the board, value:player as pieces type\n",
    "        self.last_move = -1\n",
    "\n",
    "    def move_to_location(self, move):\n",
    "        \"\"\"       \n",
    "        3*3 board's moves like:\n",
    "        6 7 8\n",
    "        3 4 5\n",
    "        0 1 2\n",
    "        and move 5's location is (1,2)\n",
    "        \"\"\"\n",
    "        h = move  // self.width\n",
    "        w = move  %  self.width\n",
    "        return [h, w]\n",
    "\n",
    "    def location_to_move(self, location):\n",
    "        if(len(location) != 2):\n",
    "            return -1\n",
    "        h = location[0]\n",
    "        w = location[1]\n",
    "        move = h * self.width + w\n",
    "        if(move not in range(self.width * self.height)):\n",
    "            return -1\n",
    "        return move\n",
    "\n",
    "    def current_state(self): \n",
    "        \"\"\"return the board state from the perspective of the current player\n",
    "        shape: 4*width*height\"\"\"\n",
    "        \n",
    "        square_state = np.zeros((4, self.width, self.height))\n",
    "        if self.states:\n",
    "            moves, players = np.array(list(zip(*self.states.items())))\n",
    "            move_curr = moves[players == self.current_player]\n",
    "            move_oppo = moves[players != self.current_player]                           \n",
    "            square_state[0][move_curr // self.width, move_curr % self.height] = 1.0\n",
    "            square_state[1][move_oppo // self.width, move_oppo % self.height] = 1.0   \n",
    "            square_state[2][self.last_move //self.width, self.last_move % self.height] = 1.0 # last move indication   \n",
    "        if len(self.states)%2 == 0:\n",
    "            square_state[3][:,:] = 1.0\n",
    "        return square_state[:,::-1,:]\n",
    "\n",
    "    def do_move(self, move):\n",
    "        self.states[move] = self.current_player\n",
    "        self.availables.remove(move)\n",
    "        self.current_player = self.players[0] if self.current_player == self.players[1] else self.players[1] \n",
    "        self.last_move = move\n",
    "\n",
    "    def has_a_winner(self):\n",
    "        width = self.width\n",
    "        height = self.height\n",
    "        states = self.states\n",
    "        n = self.n_in_row\n",
    "\n",
    "        moved = list(set(range(width * height)) - set(self.availables))\n",
    "        if(len(moved) < self.n_in_row + 2):\n",
    "            return False, -1\n",
    "\n",
    "        for m in moved:\n",
    "            h = m // width\n",
    "            w = m % width\n",
    "            player = states[m]\n",
    "\n",
    "            if (w in range(width - n + 1) and\n",
    "                len(set(states.get(i, -1) for i in range(m, m + n))) == 1):\n",
    "                return True, player\n",
    "\n",
    "            if (h in range(height - n + 1) and\n",
    "                len(set(states.get(i, -1) for i in range(m, m + n * width, width))) == 1):\n",
    "                return True, player\n",
    "\n",
    "            if (w in range(width - n + 1) and h in range(height - n + 1) and\n",
    "                len(set(states.get(i, -1) for i in range(m, m + n * (width + 1), width + 1))) == 1):\n",
    "                return True, player\n",
    "\n",
    "            if (w in range(n - 1, width) and h in range(height - n + 1) and\n",
    "                len(set(states.get(i, -1) for i in range(m, m + n * (width - 1), width - 1))) == 1):\n",
    "                return True, player\n",
    "\n",
    "        return False, -1\n",
    "\n",
    "    def game_end(self):\n",
    "        \"\"\"Check whether the game is ended or not\"\"\"\n",
    "        win, winner = self.has_a_winner()\n",
    "        if win:\n",
    "            return True, winner\n",
    "        elif not len(self.availables):#            \n",
    "            return True, -1\n",
    "        return False, -1\n",
    "\n",
    "    def get_current_player(self):\n",
    "        return self.current_player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "    def __init__(self, board, **kwargs):\n",
    "        self.board = board\n",
    "    \n",
    "    def graphic(self, board, player1, player2):\n",
    "        \"\"\"\n",
    "        Draw the board and show game info\n",
    "        \"\"\"\n",
    "        width = board.width\n",
    "        height = board.height\n",
    "\n",
    "        print(\"Player\", player1, \"with X\".rjust(3))\n",
    "        print(\"Player\", player2, \"with O\".rjust(3))\n",
    "        print()\n",
    "        for x in range(width):\n",
    "            print(\"{0:8}\".format(x), end='')\n",
    "        print('\\r\\n')\n",
    "        for i in range(height - 1, -1, -1):\n",
    "            print(\"{0:4d}\".format(i), end='')\n",
    "            for j in range(width):\n",
    "                loc = i * width + j\n",
    "                p = board.states.get(loc, -1)\n",
    "                if p == player1:\n",
    "                    print('X'.center(8), end='')\n",
    "                elif p == player2:\n",
    "                    print('O'.center(8), end='')\n",
    "                else:\n",
    "                    print('_'.center(8), end='')\n",
    "            print('\\r\\n\\r\\n')\n",
    "    \n",
    "    def start_play(self, player1, player2, start_player=0, is_shown=1):\n",
    "        \"\"\"\n",
    "        start a game between two players\n",
    "        \"\"\"\n",
    "        if start_player not in (0,1):\n",
    "            raise Exception('start_player should be 0 (player1 first) or 1 (player2 first)')\n",
    "        # Who plays first\n",
    "        self.board.init_board(start_player)\n",
    "        # set up two players [1,2]\n",
    "        p1, p2 = self.board.players\n",
    "        player1.set_player_ind(p1)\n",
    "        player2.set_player_ind(p2)\n",
    "        players = {p1:player1, p2:player2}\n",
    "        if is_shown:\n",
    "            self.graphic(self.board, player1.player, player2.player)\n",
    "        while(1):\n",
    "            current_player = self.board.get_current_player()\n",
    "            #players[1] --> player1, 2 --> player2\n",
    "            player_in_turn = players[current_player]\n",
    "            move = player_in_turn.get_action(self.board)\n",
    "            self.board.do_move(move)\n",
    "            if is_shown:\n",
    "                self.graphic(self.board, player1.player, player2.player)\n",
    "            end, winner = self.board.game_end()\n",
    "            if end:\n",
    "                if is_shown:\n",
    "                    if winner != -1:\n",
    "                        print(\"Game end. Winner is\", players[winner])\n",
    "                    else:\n",
    "                        print(\"Game end. Tie\")\n",
    "                return winner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain Search Tree class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft max function: I am confused with np.max() operation here.\n",
    "$$softmax(x) = \\frac{e^{x_i}}{\\sum_{j}e^{x_j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    probs = np.exp(x - np.max(x))\n",
    "    probs /= np.sum(probs)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a tree node class\n",
    "\n",
    "Each node in MCTS tree should contain:\n",
    "\n",
    "Q(s,a): an action value {from state s to take action a}.\n",
    "\n",
    "P(s,a): a prior probability to take action a at state s.\n",
    "\n",
    "N(s,a): number of visits from state s to take action a through MCST process.\n",
    "\n",
    "u(s,a): vist-count-adjusted score in AlphaGo Zero $$\\frac{P(s,a)}{1+N(s,a)}$$\n",
    "here the author define u(s,a) as:\n",
    "$$\\frac{\\alpha P(s,a)\\sqrt{(N(s,a)}}{1+N(s,a)}$$\n",
    "\n",
    "MCTS will select an action which returns the maximum Q(s,a) + u(s,a)\n",
    "among all actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-60b96278b930>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-60b96278b930>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    def update(self, leaf_value):\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class TreeNode(object):\n",
    "    \"\"\"A node in the MCTS tree. Each node keeps track of its own value Q.\n",
    "    prior probability P, and its visit-count-adjusted prior score u.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, parent, prior_p):\n",
    "        self._parent = parent\n",
    "        self._children = {} # a map from action to tree node\n",
    "        self._n_visits = 0\n",
    "        self._Q = 0\n",
    "        self._u = 0\n",
    "        self._P = prior_p\n",
    "    \n",
    "    def expand(self, action_priors):\n",
    "        \"\"\"Expand tree by creating new children.\n",
    "        action_priors -- output from policy function - a list of tuples\n",
    "        of actions and their prior probability according to the policy\n",
    "        function.\n",
    "        policy function f(theta) will be trained by a convolutional\n",
    "        neural network.\n",
    "        \"\"\"\n",
    "        for action, prob in action_priors:\n",
    "            if action not in self._children:\n",
    "                self._children[action] = TreeNode(self, prob)\n",
    "        \n",
    "    def select(self, c_puct):\n",
    "        \"\"\"Select action among children that gives maximum aciton value,\n",
    "        Q(s,a) + U(s,a).\n",
    "        Returns:\n",
    "        A tuple of (action, next_node)\n",
    "        \"\"\"\n",
    "        return max(self._children.items(), \n",
    "                   key=lambda act_node: act_node[1].get_value(c_puct))\n",
    "    \n",
    "    def update(self, leaf_value):\n",
    "        \"\"\"Update node Q values from leaf evaluation.\n",
    "        Arguments:\n",
    "        leaf_value -- the Q value of subtree evaluation from the current\n",
    "        player's perspective.\n",
    "        \"\"\"\n",
    "        # Count visit\n",
    "        self._n_visits += 1\n",
    "        # Update Q, a running average of values for all visits\n",
    "        self._Q += 1.0*(leaf_value - self._Q) / self._n_visits\n",
    "        \n",
    "    def update_recursive(self, leaf_value):\n",
    "        \"\"\"Like a call to update(). but applied recursively for all\n",
    "        ancestors.\n",
    "        \"\"\"\n",
    "        # if it is not root, this node's parent should be updated first\n",
    "        if self._parent:\n",
    "            self._parent.update_recursive(-leaf_value)\n",
    "        self.update(leaf_value)\n",
    "    \n",
    "    def get_value(self, c_puct):\n",
    "        \"\"\"Calculate and return the Q+U for this node: a combination\n",
    "        of leaft evaluations, Q and U.\n",
    "        c_punct -- a number in (0, inf) controlling the relative impact\n",
    "        of values, Q, and prior probability, P, on this node's score.\n",
    "        \"\"\"\n",
    "        self._u = c_puct * self._P * np.sqrt(self._parent._n_visits)/\n",
    "        (1 + self._n_visits)\n",
    "        return self._Q + self._u\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        \"\"\"Check if it is the leaf node\n",
    "        \"\"\"\n",
    "        return self._children == {}\n",
    "    \n",
    "    def is_root(self):\n",
    "        \"\"\"Check if it is the parent node\n",
    "        \"\"\"\n",
    "        return self._parent is None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search\n",
    "\n",
    "### get_move_probs()\n",
    "act_probs = $$softmax(\\frac{1}{temp}*log(N+1e-10))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-17-51f747eb3b4e>, line 81)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-51f747eb3b4e>\"\u001b[0;36m, line \u001b[0;32m81\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class MCTS(object):\n",
    "    \"\"\"A simple implementation of Monte Carlo Tree Search\n",
    "    \"\"\"\n",
    "    def __init__(self, policy_value_fn, c_puct=5, n_playout=10000):\n",
    "        \"\"\"Arguments:\n",
    "        policy_value_fn -- a function that takes in a board state and outputs\n",
    "        a list of (action, probability) tuples and also a score in [-1,1] (i.e.\n",
    "        the expected value of the end game score from the current players's\n",
    "        perspective) for the current player.\n",
    "        c_puct -- a number in (0, inf) that controls how quickly exploration\n",
    "        converges to the maximum-value policy, where a higher value means relying\n",
    "        on the prior more\n",
    "        \"\"\"\n",
    "        self._root = TreeNode(None, 1.0)\n",
    "        self._policy = policy_value_fn\n",
    "        self._c_puct = c_puct\n",
    "        self._n_playout = n_playout\n",
    "    \n",
    "    def _playout(self, state):\n",
    "        \"\"\"Run a single playout from the root to the leaf, getting a value at\n",
    "        the leaf and propagating it back through its parents. State is modified\n",
    "        in-place, so a copy must be provided.\n",
    "        Arguments:\n",
    "        state -- a copy of the state.\n",
    "        \"\"\"\n",
    "        node = self._root\n",
    "        while(1):\n",
    "            if node.is_leaf():\n",
    "                break\n",
    "            # Greedily select next move.\n",
    "            # choose Max Q + U action\n",
    "            action, node = node.select(self._c_puct)\n",
    "            #do_move in game class\n",
    "            state.do_move(action)\n",
    "            \n",
    "        # Evaluate the leaf using a network which outputs a list of (action, probability)\n",
    "        # tuples p and also a score v in [-1,1] for the current player.\n",
    "        action_probs, leaf_value = self._policy(state)\n",
    "        # Check for end of game\n",
    "        end, winner = state.game_end()\n",
    "        if not end:\n",
    "            # expand tree by creating new children\n",
    "            node.expand(action_probs)\n",
    "        else:\n",
    "            # for end state, return the true leaf_value\n",
    "            if winner == -1: #tie\n",
    "                leaf_value = 0.0\n",
    "            else:\n",
    "                leaf_value = 1.0 if winner == state.get_current_player() else -1.0\n",
    "        \n",
    "        # Update value and visit count of nodes in this traversal\n",
    "        node.update_recursive(-leaf_value)\n",
    "    \n",
    "    def get_move_probs(self, state, temp=1e-3):\n",
    "        \"\"\"Run all playouts sequentially and returns the available action and\n",
    "        their corresponding probabilities\n",
    "        Arguments:\n",
    "        state -- the current state, including both game state and the current player\n",
    "        temp -- temperature parameter in (0,1] that controls the level of exploration\n",
    "        Returns:\n",
    "        the available actions and the corresponding probabilities\n",
    "        \"\"\"\n",
    "        for n in range(self._n_playout):\n",
    "            state_copy = copy.deepcopy(state)\n",
    "            self._playout(state_copy)\n",
    "        \n",
    "        # calc the move probabilities based on the visit counts at the root node\n",
    "        act_visits = [(act, node._n_visits) for act, node in self._root._children.items()]\n",
    "        acts, visits = zip(*act_visits)\n",
    "        act_probs = softmax(1.0/temp * np.log(np.array(visits) + 1e-10))\n",
    "        \n",
    "        return acts, act_probs\n",
    "    \n",
    "    def update_with_move(self, last_move):\n",
    "        \"\"\"Step forward in the tree, keeping everything we already know\n",
    "        about the subtree. To avoid recursive update to change value of\n",
    "        ancestors later.这样新的一步就是起点，对着新的起点开始，之前几步的value不\n",
    "        再去改变。\n",
    "        \"\"\"\n",
    "        if last_move in self._root._children:\n",
    "            self._root = self._root._children[last_move]\n",
    "            self._root._parent = None\n",
    "        else:\n",
    "            self._root = TreeNode(None, 1.0)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"MCTS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS Player\n",
    "\n",
    "get_move_probs -> _playout --> update_recursive\n",
    "\n",
    "for state s repeat the following procedure n times:\n",
    "1. select action and move to this action until it is a leaf\n",
    "2. input leaf state to network and return action prob + leaf value\n",
    "3. check if such state is end of game\n",
    "4. if not: expand the leaf\n",
    "5. if is: set leaf value\n",
    "6. update the leaf_value\n",
    "end\n",
    "\n",
    "find action probs based on number of visit from state s to children nodes\n",
    "return acts and probs\n",
    "\n",
    "### get_action \n",
    "input current board state,\n",
    "\n",
    "if there are sensible moves:\n",
    "1. get_move_probs\n",
    "2. choice the action\n",
    "3. update the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-18-d0a6bc56ee6f>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-d0a6bc56ee6f>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    def set_player_ind(self, p):\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class MCTSPlayer(object):\n",
    "    \"\"\"AI player based on MCTS\"\"\"\n",
    "    def __init__(self, policy_value_function, c_puct=5, n_playout=2000,\n",
    "                is_selfplay=0):\n",
    "        self.mcts = MCTS(policy_value_function, c_puct, n_playout)\n",
    "        self._is_selfplay = is_selfplay\n",
    "    \n",
    "    def set_player_ind(self, p):\n",
    "        self.player = p\n",
    "    \n",
    "    def reset_player(self):\n",
    "        self.mcts.update_with_move(-1)\n",
    "        \n",
    "    def get_action(self, board, temp=1e-3, return_prob=0):\n",
    "        sensible_moves = board.availables\n",
    "        # the pi vector returned by MCTS as in the alphago zero\n",
    "        move_probs = np.zeros(board.width*board.height)\n",
    "        if len(sensible_moves) > 0:\n",
    "            acts, probs = self.mcts.get_move_probs(board, temp)\n",
    "            move_probs[list(acts)] = probs\n",
    "            if self._is_selfplay:\n",
    "                # add dirichlet noise for exploration\n",
    "                move = np.random.choice(acts, p=0.75*probs + \n",
    "                                        0.25*np.random.dirichlet(0.3*np.ones(len(probs))))\n",
    "                self.mcts.update_with_move(move) # update the root node and reuse the search tree\n",
    "            else:\n",
    "                # with the default temp=1e-3, this is almost equivalent to choosing the move \n",
    "                #with the highest prob\n",
    "                move = np.random.choice(acts, p=probs)\n",
    "                # reset the root node\n",
    "                self.mcts.update_with_move(-1)\n",
    "                \n",
    "            if return_prob:\n",
    "                return move, move_probs\n",
    "            else:\n",
    "                return move\n",
    "        else:\n",
    "            print(\"Warning: the board is full\")\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"MCTS {}\".format(self.player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:p35]",
   "language": "python",
   "name": "conda-env-p35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
